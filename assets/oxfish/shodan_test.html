<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Ernesto" />


<title>Endogenous Discovery of Policies</title>

<script src="shodan_test_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="shodan_test_files/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="shodan_test_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="shodan_test_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="shodan_test_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="shodan_test_files/navigation-1.1/tabsets.js"></script>




<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Endogenous Discovery of Policies</h1>
<h4 class="author"><em>Ernesto</em></h4>
<h4 class="date"><em>January 6, 2017</em></h4>

</div>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-58894263-1', 'auto');
  ga('send', 'pageview');

</script>
<div id="open-vs-closed-loop" class="section level2">
<h2>Open vs Closed Loop</h2>
<p>We have an agent-based model of fishing. We can use it to predict the consequence of imposing a policy. So far we used this for two different tasks:</p>
<ul>
<li>Scenario Evaluation</li>
<li>Policy Optimization</li>
</ul>
<p>By <em>scenario evaluation</em> I mean taking the model, inputting a policy and see what it does. By <em>policy optimization</em> I mean looking for the best parameters of a given rule (say, what is the best quota amount to impose for each species of fish if we want to maximize cash returns over 20 years).</p>
<p>The main weakness of this approach is that it is fundamentally <strong>open loop</strong>: the rule is set at the beginning and remains constant throughout the run. This means that the policy is brittle to shocks as it never uses any information that is gathered while the model runs.</p>
<div class="figure">
<img src="openloop.png" />

</div>
<p>What we would like instead is <strong>closed loop</strong> rules where the information generated by the model can be used to adjust the policy (for example sensing a recruitment boom the quotas could be increased). This was our old idea of having a “policy-maker” agent within the model</p>
<div class="figure">
<img src="closedloop.png" />

</div>
<p>For our agent-based model there are two ways of accomplishing this:</p>
<ul>
<li>Policy search</li>
<li>Value function approximation</li>
</ul>
<p><em>Policy search</em> is what we did when simulating <a href="http://carrknight.github.io/assets/oxfish/adaptive_tax.html">adaptive taxation</a>: the modeler comes up with an adaptive policy (in that case a threshold tax) and then the optimizer searches for the policy parameters much like the open loop example.</p>
<p>The hard case however is when we don’t really know a priori what would be a good policy rule. We might have some indicators about the fishery and some actions we can take but no idea on how to connect the two. In a very simple mathematical model we could map indicators to actions by dynamic programming, solving the Bellman equation. Agent based models however aren’t amenable to that work as the transition probabilities are hard to compute. We could run the model a large number of times and discover the transition probabilities but that would also be hard if we only have a few indicators.<br />
What we do instead is to try and approximate the dynamic programming’s value function directly by running the model many times and iteratively improve our strategy.</p>
<p>For example, imagine that our fishery indicators are the biomass left and the month of the year. Our only action is whether to open the fishery that month or not. The only reward we care about is the amount of cash made by fishers. If we run the model randomly opening and closing the season we might produce a data-set like this:</p>
<table>
<thead>
<tr class="header">
<th>Biomass</th>
<th>Month</th>
<th>Action</th>
<th>Reward</th>
<th>Biomass after</th>
<th>Month after</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100</td>
<td>1</td>
<td>open</td>
<td>100</td>
<td>80</td>
<td>2</td>
</tr>
<tr class="even">
<td>80</td>
<td>2</td>
<td>close</td>
<td>0</td>
<td>80</td>
<td>3</td>
</tr>
<tr class="odd">
<td>80</td>
<td>3</td>
<td>open</td>
<td>90</td>
<td>65</td>
<td>4</td>
</tr>
<tr class="even">
<td>65</td>
<td>4</td>
<td>open</td>
<td>80</td>
<td>30</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>We can predict the reward produced by an action current indicators by running the following regression <span class="math display">\[ R = \beta_0 + \beta_1 \text{Biomass} + \beta_2 \text{Month} \]</span> twice, one for the observations where we pick action <code>open</code> and one where we pick <code>close</code>. Then in principle we could choose each month the action which the regressions predict to carry the highest reward.</p>
<p>The problem with this naive approach is that it is very short-term oriented. In this example the rewards are always 0 when the fishery is closed so that it will always be better to keep the fishery open. Rather than predicting the current reward alone then we would rather predict the sum of all future rewards (discounted by <span class="math inline">\(\gamma\)</span>) after taking an action given certain indicators: <span class="math display">\[ R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots \]</span> This however is hard to do because the rewards in the future depend on other actions we will take later.</p>
<p>We want to focus on the value function <span class="math inline">\(V(S)\)</span> which is the sum of all rewards we would achieve if we see indicators <span class="math inline">\(S\)</span> and take the “optimal” sequence of choices from then on. <span class="math display">\[ V(S) = \max_{a_t,a_{t+1},\dots} R_t(a_t) + \gamma R_{t+1}(a_{t+1}) + \dots  \]</span> And as with dynamic programming we can turn this recursively as: <span class="math display">\[ V(S) = \max_{a_t} R_t(a_t) + \gamma V(S_{t+1}) \]</span> The value function is unobservable but we can start with a guess <span class="math inline">\(\bar V\)</span>, acting according to that guess and use regressions to improve it over time. We are trying to approximate the value function as follows: <span class="math display">\[ V = \beta_0 + \beta_1 \text{Biomass} + \beta_2 \text{Month} \]</span> And we run regressions where the <span class="math inline">\(y\)</span> is always the observed reward plus our guess <span class="math inline">\(R + \gamma \hat V\)</span> where <span class="math inline">\(\hat V\)</span> updates each time we have a new observation. This is a biased but consistent estimator for <span class="math inline">\(V\)</span>.</p>
<p>This approach is called approximate dynamic programming or reinforcement learning and we are going to use it to try and discover endogenously good policies.</p>
</div>
<div id="discovering-policies" class="section level2">
<h2>Discovering policies</h2>
<div id="bayesian-optimization" class="section level3">
<h3>Bayesian Optimization</h3>
<p>Imagine our usual baseline scenario where there is a bunch of fish growing logistically all over the map and 300 fishers catching it. Because the fishers are too many the biomass quickly dies off.</p>
<video controls width="1000" height="600"  poster="http://thumbs.gfycat.com/ImpossiblePoliticalAmericanalligator-poster.jpg">
<source src="http://fat.gfycat.com/ImpossiblePoliticalAmericanalligator.webm" type="video/webm">
<source src="http://giant.gfycat.com/ImpossiblePoliticalAmericanalligator.mp4" type="video/mp4">
</video>
<p><img src="shodan_test_files/figure-html/unnamed-chunk-1-1.png" width="1000" /></p>
<p>Imagine we wanted to maximize 20 years earnings. We can go the usual route and look for the quota amount that maximizes 20 year cash-flow. That’s a classic open-loop policy optimization which we can feed to our bayesian optimizer. This generates a Bayesian posterior like this:</p>
<p><img src="shodan_test_files/figure-html/unnamed-chunk-2-1.png" width="1000" /></p>
<p>The maximum of the posterior is 712,038 units of fish a year. If we run a simulation with this policy we get the following result:</p>
<p><img src="shodan_test_files/figure-html/unnamed-chunk-3-1.png" width="1000" /></p>
<p>Basically the quota is just enough to consume the biomass over 20 years; this maximizes our only objective which is fishers’ cashflow.</p>
</div>
<div id="value-function-approximation" class="section level3">
<h3>Value Function Approximation</h3>
<p>Now imagine the same scenario except that we aren’t able to impose quotas (for whatever reason). Our only action, each month, is whether to open or close the fishery for the next 30 days.</p>
<p>Imagine we are looking at two indicators:</p>
<ol style="list-style-type: decimal">
<li>Biomass</li>
<li>Months left before the end</li>
</ol>
<p>Intuitively there must be a way to look at biomass and months left and decide whether to open the fishery or not. When biomass is abundant we probably want to open the fishery; likewise when there are only a few months left before the 20 year mark we want to open the fishery to catch as much as possible before the deadline.</p>
<p>We want however the computer learn this mapping on its own. We set up a value approximation as follows: <span class="math display">\[
\left\{\begin{matrix}
V^{\text{open}} = \beta_0 + \beta_1 \text{Biomass} + \beta_2 \text{Months} &amp;  \text{open} \\ 
V^{\text{close}} = \beta_3 + \beta_4 \text{Biomass} + \beta_5 \text{Months} &amp;  \text{closed}
\end{matrix}\right.
\]</span> Where, each month we compute <span class="math inline">\(V^{\text{open}}\)</span> and <span class="math inline">\(V^{\text{close}}\)</span> and allow fishing that month whenever <span class="math inline">\(V^{\text{open}}&gt; V^{\text{close}}\)</span>. We need to discover the various <span class="math inline">\(\beta\)</span> by running the model many times, observing transitions and running regressions again.</p>
<p>In practice however it turns out that more complicated approximations work best so that for this example we used a Fourier basis. The procedure is the same except the regressions are over a sum of sinusoidals rather than using features directly.</p>
<p>We run the model multiple times, improving our <span class="math inline">\(\beta\)</span> at each observation and so approximate value function which in turn leads to better actions. After 150 episodes observed, we use the approximate value function we obtained on a full run to test its effectivness. The following figure shows a sample run (using the same random seed as the bayesian optimizer case)</p>
<p><img src="shodan_test_files/figure-html/biomass_opt-1.png" width="1000" /></p>
<p>The policy learned keeps the fishery mostly open for the first few years until the biomass is consumed, this is followed by a period of about 10 years where the seasons are extremely short followed again by a period where the fishery is left mostly open. It’s an irregular boom-bust cycle but one that in fact performs slightly better than the fixed quota system in terms of 20 years cash flow.</p>
<p>Knowing biomass however is kind of cheating: it’s an almost perfect indicator of the state of the fishery (its only weakness is not considering spatial distribution). A more interesting problem is how would we open and close the fishery if we could only look at human indicators. For example imagine that we can only observe the following:</p>
<ol style="list-style-type: decimal">
<li>Average Yearly Returns</li>
<li>Average Distance from Port (from last tow)</li>
<li>Months Left</li>
</ol>
<p>We can do the same procedure and train the policy except using different indicators. The result is a similar boom-bust cycle over 20 years except more pronounced with biomass recovering above the initial values towards the middle of the run:<br />
<img src="shodan_test_files/figure-html/cash_opt-1.png" width="1000" /></p>
<p>So there we have it, we design a feedback control loop to maximize profits over our complicated agent-based model.<br />
For this particular example the trained controller works better than quotas in terms of profits although very close.</p>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th>Reward</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Quota</td>
<td>412056.27</td>
</tr>
<tr class="even">
<td>Biomass controller</td>
<td>458028.11</td>
</tr>
<tr class="odd">
<td>Cash and distance controller</td>
<td>448624.45</td>
</tr>
</tbody>
</table>
<p>This somewhat hides the uglyness of the procedure: so far it is extremely brittle to its learning parameters, to its basis function, to episodes we train it against and instead of just converging to a good policy it has a tendency of cycling between good control and bad control.<br />
This is partially because i am very new at this technique but I think we should understand that reinforcement learning is hard and takes time and a lot of effort to get right.</p>
</div>
<div id="streching-beyond" class="section level3">
<h3>Streching beyond</h3>
<p>What if we take the rules we computed and strech them over a much longer 80 year period? Which one is maximizing cash-flow then?</p>
<p>Surprisingly even though the reinforcement learning agent has been trained to maximize profits over 20 years and one of the two indicators is “months left before the end” it adapts quite nicely to the longer time frame:</p>
<p><img src="shodan_test_files/figure-html/biomass_opt80-1.png" width="1000" /></p>
<p>We can also train a biomass controller that doesn’t use months left as an indicator and feed him training episodes of different length, which generates control dynamics such as this:</p>
<p><img src="shodan_test_files/figure-html/biomass_opt_general-1.png" width="1000" /></p>
<p>which looks better even though it tends to make the same amount of money overall.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
